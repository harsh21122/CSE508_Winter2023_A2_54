{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/harsh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import string\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(folder, new_folder):\n",
    "    files = os.listdir(folder)\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        path = os.path.join(os.getcwd(), folder, file)\n",
    "        with open(path) as fp:\n",
    "            \n",
    "            soup = BeautifulSoup(fp, 'html.parser')\n",
    "            if count < 5:\n",
    "                print(\"\\033[1m\" + \"Before Extraction : \" + \"\\033[0m\", soup)\n",
    "            text = soup.findAll(\"text\")[0].text\n",
    "            title = soup.findAll(\"title\")[0].text\n",
    "            final_text = title + \" \" + text\n",
    "            if count < 5:\n",
    "                print(\"\\033[1m\" + \"After Extraction : \" + \"\\033[0m\" , final_text)\n",
    "                count += 1\n",
    "            \n",
    "        new_file_path = os.path.join(os.getcwd(), new_folder, file)\n",
    "        with open(new_file_path, \"w\") as fw:\n",
    "            fw.write(final_text)\n",
    "            fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  already exists.\n",
      "\u001b[1mBefore Extraction : \u001b[0m <doc>\n",
      "<docno>\n",
      "1223\n",
      "</docno>\n",
      "<title>\n",
      "inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets, in proximity to the ground .\n",
      "</title>\n",
      "<author>\n",
      "strand,t.\n",
      "</author>\n",
      "<biblio>\n",
      "j. ae. scs. 1962, 170.\n",
      "</biblio>\n",
      "<text>\n",
      "  the inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets impinging orthogonally on the ground is presented\n",
      "using conformal mapping methods .\n",
      "  it is shown that the thrust of a solid jet at constant power\n",
      "initially decreases as the ground is approached .  the magnitude\n",
      "of the thrust out of ground effect is regained only at a very\n",
      "low height-to-jet width ratio (approximately 0.55) .  the maximuin\n",
      "decrease is about 6 percent .  the ground effect on solid\n",
      "jets is thus largely unfavorable .\n",
      "</text>\n",
      "</doc>\n",
      "\n",
      "\u001b[1mAfter Extraction : \u001b[0m \n",
      "inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets, in proximity to the ground .\n",
      " \n",
      "  the inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets impinging orthogonally on the ground is presented\n",
      "using conformal mapping methods .\n",
      "  it is shown that the thrust of a solid jet at constant power\n",
      "initially decreases as the ground is approached .  the magnitude\n",
      "of the thrust out of ground effect is regained only at a very\n",
      "low height-to-jet width ratio (approximately 0.55) .  the maximuin\n",
      "decrease is about 6 percent .  the ground effect on solid\n",
      "jets is thus largely unfavorable .\n",
      "\n",
      "\u001b[1mBefore Extraction : \u001b[0m <doc>\n",
      "<docno>\n",
      "1011\n",
      "</docno>\n",
      "<title>\n",
      "free-flight measurements of the static and dynamic\n",
      "</title>\n",
      "<author>\n",
      "</author>\n",
      "<biblio>\n",
      "</biblio>\n",
      "<text>\n",
      "  charts have been prepared relating the thermodynamic properties of\n",
      "air in chemical equilibrium for temperatures to 15,000 k and for pressures\n",
      "from 10 to 10 atmospheres .  also included are charts showing\n",
      "the composition of air, the isentropic exponent, and the speed of\n",
      "sound .  these charts are based on thermodynamic data calculated by the\n",
      "national bureau of standards .\n",
      "</text>\n",
      "</doc>\n",
      "\n",
      "\u001b[1mAfter Extraction : \u001b[0m \n",
      "free-flight measurements of the static and dynamic\n",
      " \n",
      "  charts have been prepared relating the thermodynamic properties of\n",
      "air in chemical equilibrium for temperatures to 15,000 k and for pressures\n",
      "from 10 to 10 atmospheres .  also included are charts showing\n",
      "the composition of air, the isentropic exponent, and the speed of\n",
      "sound .  these charts are based on thermodynamic data calculated by the\n",
      "national bureau of standards .\n",
      "\n",
      "\u001b[1mBefore Extraction : \u001b[0m <doc>\n",
      "<docno>\n",
      "795\n",
      "</docno>\n",
      "<title>\n",
      "the operation of the npl 18in x 14in. wind tunnel in the transonic speed\n",
      " range .\n",
      "</title>\n",
      "<author>\n",
      "hall, i.m.\n",
      "</author>\n",
      "<biblio>\n",
      "a.r.c. c.p. 338, january 1957 .\n",
      "</biblio>\n",
      "<text>\n",
      "a brief description of the slotted liners used is given together with\n",
      "the power requirements and some flow surveys .\n",
      "some observations are made on wall interference on a half-model of a\n",
      "swept wing tested in the wind tunnel .\n",
      "</text>\n",
      "</doc>\n",
      "\n",
      "\u001b[1mAfter Extraction : \u001b[0m \n",
      "the operation of the npl 18in x 14in. wind tunnel in the transonic speed\n",
      " range .\n",
      " \n",
      "a brief description of the slotted liners used is given together with\n",
      "the power requirements and some flow surveys .\n",
      "some observations are made on wall interference on a half-model of a\n",
      "swept wing tested in the wind tunnel .\n",
      "\n",
      "\u001b[1mBefore Extraction : \u001b[0m <doc>\n",
      "<docno>\n",
      "553\n",
      "</docno>\n",
      "<title>\n",
      "ablation of glassy materials around blunt bodies of\n",
      "revolution .\n",
      "</title>\n",
      "<author>\n",
      "hidalgo,h.\n",
      "</author>\n",
      "<biblio>\n",
      "ars j. 30, 1960.\n",
      "</biblio>\n",
      "<text>\n",
      "  the steady-state equations of motion for a\n",
      "thin layer of an incompressible glassy material on the\n",
      "surface of an ablating and radiating blunt\n",
      "body are reduced to a first-order ordinary differential\n",
      "equation which is integrated numerically .\n",
      "this solution is coupled with the solution of the air\n",
      "boundary layer for both laminar and turbulent\n",
      "heat transfer with or without mass vaporization of\n",
      "the ablating material .  the distribution of the\n",
      "effective energy of ablation around the body is thus\n",
      "obtained for a cone cylinder with a hemispherical\n",
      "cap that re-enters the atmosphere at hypersonic\n",
      "flight speeds, and has quartz as the ablating\n",
      "material .  it is found that the ablation process from\n",
      "turbulent heating is more efficient than from\n",
      "the laminar case because of increased vaporization .\n",
      "this solution of the equations of motion at the\n",
      "stagnation point has been verified by are wind tunnel\n",
      "experiments .  the present state of development\n",
      "of the are wind tunnel does not permit its use for\n",
      "experimental investigations of ablation around\n",
      "blunt bodies under turbulent heating .\n",
      "</text>\n",
      "</doc>\n",
      "\n",
      "\u001b[1mAfter Extraction : \u001b[0m \n",
      "ablation of glassy materials around blunt bodies of\n",
      "revolution .\n",
      " \n",
      "  the steady-state equations of motion for a\n",
      "thin layer of an incompressible glassy material on the\n",
      "surface of an ablating and radiating blunt\n",
      "body are reduced to a first-order ordinary differential\n",
      "equation which is integrated numerically .\n",
      "this solution is coupled with the solution of the air\n",
      "boundary layer for both laminar and turbulent\n",
      "heat transfer with or without mass vaporization of\n",
      "the ablating material .  the distribution of the\n",
      "effective energy of ablation around the body is thus\n",
      "obtained for a cone cylinder with a hemispherical\n",
      "cap that re-enters the atmosphere at hypersonic\n",
      "flight speeds, and has quartz as the ablating\n",
      "material .  it is found that the ablation process from\n",
      "turbulent heating is more efficient than from\n",
      "the laminar case because of increased vaporization .\n",
      "this solution of the equations of motion at the\n",
      "stagnation point has been verified by are wind tunnel\n",
      "experiments .  the present state of development\n",
      "of the are wind tunnel does not permit its use for\n",
      "experimental investigations of ablation around\n",
      "blunt bodies under turbulent heating .\n",
      "\n",
      "\u001b[1mBefore Extraction : \u001b[0m <doc>\n",
      "<docno>\n",
      "761\n",
      "</docno>\n",
      "<title>\n",
      "buckling of sandwich under normal pressure .\n",
      "</title>\n",
      "<author>\n",
      "yao,j.c.\n",
      "</author>\n",
      "<biblio>\n",
      "j. ae. scs. 29, 1962, 264.\n",
      "</biblio>\n",
      "<text>\n",
      "  a theoretical study is made of the buckling of a sandwich sphere\n",
      "comprised of a core layer of low-modulus material and two thin\n",
      "facing layers of higher modulus material .  the solution for the\n",
      "buckling resistance of the sphere under normal external pressure\n",
      "is obtained by linearized theory, and is reducible to the classical\n",
      "solution for monocoque spherical shells .  critical buckling pressures\n",
      "are calculated for various radius-thickness ratios and sphere\n",
      "materials .\n",
      "</text>\n",
      "</doc>\n",
      "\n",
      "\u001b[1mAfter Extraction : \u001b[0m \n",
      "buckling of sandwich under normal pressure .\n",
      " \n",
      "  a theoretical study is made of the buckling of a sandwich sphere\n",
      "comprised of a core layer of low-modulus material and two thin\n",
      "facing layers of higher modulus material .  the solution for the\n",
      "buckling resistance of the sphere under normal external pressure\n",
      "is obtained by linearized theory, and is reducible to the classical\n",
      "solution for monocoque spherical shells .  critical buckling pressures\n",
      "are calculated for various radius-thickness ratios and sphere\n",
      "materials .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_folder = 'Dataset'\n",
    "try:\n",
    "    os.mkdir(new_folder)\n",
    "except:\n",
    "    print(new_folder, \" already exists.\")\n",
    "file_map = extract_data('CSE508_Winter2023_Dataset', new_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, flag):\n",
    "    if flag:\n",
    "        print(\"\\033[1m\" + \"Before lower case text : \" + \"\\033[0m\" , text)\n",
    "        \n",
    "    text = text.lower()\n",
    "    if flag:\n",
    "        print(\"\\033[1m\" + \"After lower case text and before tokenization : \"+ \"\\033[0m\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    if flag:\n",
    "        print(\"\\033[1m\" + \"After tokenization and before stopwords removal : \"+ \"\\033[0m\", tokens)\n",
    "    \n",
    "    final = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    if flag:\n",
    "        print(\"\\033[1m\" + \"After stopwords removal and before punctuations removal : \"+ \"\\033[0m\", final)\n",
    "\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    if flag:\n",
    "        print(\"\\033[1m\" + \"After punctuations and before blank space token removal : \"+ \"\\033[0m\", tokens)\n",
    "     \n",
    "    final = [word for word in tokens if len(re.findall(r'\\s+', word)) == 0]\n",
    "    \n",
    "    if flag:\n",
    "        print(\"\\033[1m\" + \"After blank space token removal : \"+ \"\\033[0m\", final)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "Printing Statement : 1\n",
      "\u001b[1mBefore lower case text : \u001b[0m \n",
      "inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets, in proximity to the ground .\n",
      " \n",
      "  the inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets impinging orthogonally on the ground is presented\n",
      "using conformal mapping methods .\n",
      "  it is shown that the thrust of a solid jet at constant power\n",
      "initially decreases as the ground is approached .  the magnitude\n",
      "of the thrust out of ground effect is regained only at a very\n",
      "low height-to-jet width ratio (approximately 0.55) .  the maximuin\n",
      "decrease is about 6 percent .  the ground effect on solid\n",
      "jets is thus largely unfavorable .\n",
      "\n",
      "\u001b[1mAfter lower case text and before tokenization : \u001b[0m \n",
      "inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets, in proximity to the ground .\n",
      " \n",
      "  the inviscid-incompressible-flow theory of static two-dimensional\n",
      "solid jets impinging orthogonally on the ground is presented\n",
      "using conformal mapping methods .\n",
      "  it is shown that the thrust of a solid jet at constant power\n",
      "initially decreases as the ground is approached .  the magnitude\n",
      "of the thrust out of ground effect is regained only at a very\n",
      "low height-to-jet width ratio (approximately 0.55) .  the maximuin\n",
      "decrease is about 6 percent .  the ground effect on solid\n",
      "jets is thus largely unfavorable .\n",
      "\n",
      "\u001b[1mAfter tokenization and before stopwords removal : \u001b[0m ['inviscid-incompressible-flow', 'theory', 'of', 'static', 'two-dimensional', 'solid', 'jets', ',', 'in', 'proximity', 'to', 'the', 'ground', '.', 'the', 'inviscid-incompressible-flow', 'theory', 'of', 'static', 'two-dimensional', 'solid', 'jets', 'impinging', 'orthogonally', 'on', 'the', 'ground', 'is', 'presented', 'using', 'conformal', 'mapping', 'methods', '.', 'it', 'is', 'shown', 'that', 'the', 'thrust', 'of', 'a', 'solid', 'jet', 'at', 'constant', 'power', 'initially', 'decreases', 'as', 'the', 'ground', 'is', 'approached', '.', 'the', 'magnitude', 'of', 'the', 'thrust', 'out', 'of', 'ground', 'effect', 'is', 'regained', 'only', 'at', 'a', 'very', 'low', 'height-to-jet', 'width', 'ratio', '(', 'approximately', '0.55', ')', '.', 'the', 'maximuin', 'decrease', 'is', 'about', '6', 'percent', '.', 'the', 'ground', 'effect', 'on', 'solid', 'jets', 'is', 'thus', 'largely', 'unfavorable', '.']\n",
      "\u001b[1mAfter stopwords removal and before punctuations removal : \u001b[0m ['inviscid-incompressible-flow', 'theory', 'static', 'two-dimensional', 'solid', 'jets', ',', 'proximity', 'ground', '.', 'inviscid-incompressible-flow', 'theory', 'static', 'two-dimensional', 'solid', 'jets', 'impinging', 'orthogonally', 'ground', 'presented', 'using', 'conformal', 'mapping', 'methods', '.', 'shown', 'thrust', 'solid', 'jet', 'constant', 'power', 'initially', 'decreases', 'ground', 'approached', '.', 'magnitude', 'thrust', 'ground', 'effect', 'regained', 'low', 'height-to-jet', 'width', 'ratio', '(', 'approximately', '0.55', ')', '.', 'maximuin', 'decrease', '6', 'percent', '.', 'ground', 'effect', 'solid', 'jets', 'thus', 'largely', 'unfavorable', '.']\n",
      "\u001b[1mAfter punctuations and before blank space token removal : \u001b[0m ['inviscid-incompressible-flow', 'theory', 'of', 'static', 'two-dimensional', 'solid', 'jets', 'in', 'proximity', 'to', 'the', 'ground', 'the', 'inviscid-incompressible-flow', 'theory', 'of', 'static', 'two-dimensional', 'solid', 'jets', 'impinging', 'orthogonally', 'on', 'the', 'ground', 'is', 'presented', 'using', 'conformal', 'mapping', 'methods', 'it', 'is', 'shown', 'that', 'the', 'thrust', 'of', 'a', 'solid', 'jet', 'at', 'constant', 'power', 'initially', 'decreases', 'as', 'the', 'ground', 'is', 'approached', 'the', 'magnitude', 'of', 'the', 'thrust', 'out', 'of', 'ground', 'effect', 'is', 'regained', 'only', 'at', 'a', 'very', 'low', 'height-to-jet', 'width', 'ratio', 'approximately', '0.55', 'the', 'maximuin', 'decrease', 'is', 'about', '6', 'percent', 'the', 'ground', 'effect', 'on', 'solid', 'jets', 'is', 'thus', 'largely', 'unfavorable']\n",
      "\u001b[1mAfter blank space token removal : \u001b[0m ['inviscid-incompressible-flow', 'theory', 'of', 'static', 'two-dimensional', 'solid', 'jets', 'in', 'proximity', 'to', 'the', 'ground', 'the', 'inviscid-incompressible-flow', 'theory', 'of', 'static', 'two-dimensional', 'solid', 'jets', 'impinging', 'orthogonally', 'on', 'the', 'ground', 'is', 'presented', 'using', 'conformal', 'mapping', 'methods', 'it', 'is', 'shown', 'that', 'the', 'thrust', 'of', 'a', 'solid', 'jet', 'at', 'constant', 'power', 'initially', 'decreases', 'as', 'the', 'ground', 'is', 'approached', 'the', 'magnitude', 'of', 'the', 'thrust', 'out', 'of', 'ground', 'effect', 'is', 'regained', 'only', 'at', 'a', 'very', 'low', 'height-to-jet', 'width', 'ratio', 'approximately', '0.55', 'the', 'maximuin', 'decrease', 'is', 'about', '6', 'percent', 'the', 'ground', 'effect', 'on', 'solid', 'jets', 'is', 'thus', 'largely', 'unfavorable']\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "Printing Statement : 2\n",
      "\u001b[1mBefore lower case text : \u001b[0m \n",
      "free-flight measurements of the static and dynamic\n",
      " \n",
      "  charts have been prepared relating the thermodynamic properties of\n",
      "air in chemical equilibrium for temperatures to 15,000 k and for pressures\n",
      "from 10 to 10 atmospheres .  also included are charts showing\n",
      "the composition of air, the isentropic exponent, and the speed of\n",
      "sound .  these charts are based on thermodynamic data calculated by the\n",
      "national bureau of standards .\n",
      "\n",
      "\u001b[1mAfter lower case text and before tokenization : \u001b[0m \n",
      "free-flight measurements of the static and dynamic\n",
      " \n",
      "  charts have been prepared relating the thermodynamic properties of\n",
      "air in chemical equilibrium for temperatures to 15,000 k and for pressures\n",
      "from 10 to 10 atmospheres .  also included are charts showing\n",
      "the composition of air, the isentropic exponent, and the speed of\n",
      "sound .  these charts are based on thermodynamic data calculated by the\n",
      "national bureau of standards .\n",
      "\n",
      "\u001b[1mAfter tokenization and before stopwords removal : \u001b[0m ['free-flight', 'measurements', 'of', 'the', 'static', 'and', 'dynamic', 'charts', 'have', 'been', 'prepared', 'relating', 'the', 'thermodynamic', 'properties', 'of', 'air', 'in', 'chemical', 'equilibrium', 'for', 'temperatures', 'to', '15,000', 'k', 'and', 'for', 'pressures', 'from', '10', 'to', '10', 'atmospheres', '.', 'also', 'included', 'are', 'charts', 'showing', 'the', 'composition', 'of', 'air', ',', 'the', 'isentropic', 'exponent', ',', 'and', 'the', 'speed', 'of', 'sound', '.', 'these', 'charts', 'are', 'based', 'on', 'thermodynamic', 'data', 'calculated', 'by', 'the', 'national', 'bureau', 'of', 'standards', '.']\n",
      "\u001b[1mAfter stopwords removal and before punctuations removal : \u001b[0m ['free-flight', 'measurements', 'static', 'dynamic', 'charts', 'prepared', 'relating', 'thermodynamic', 'properties', 'air', 'chemical', 'equilibrium', 'temperatures', '15,000', 'k', 'pressures', '10', '10', 'atmospheres', '.', 'also', 'included', 'charts', 'showing', 'composition', 'air', ',', 'isentropic', 'exponent', ',', 'speed', 'sound', '.', 'charts', 'based', 'thermodynamic', 'data', 'calculated', 'national', 'bureau', 'standards', '.']\n",
      "\u001b[1mAfter punctuations and before blank space token removal : \u001b[0m ['free-flight', 'measurements', 'of', 'the', 'static', 'and', 'dynamic', 'charts', 'have', 'been', 'prepared', 'relating', 'the', 'thermodynamic', 'properties', 'of', 'air', 'in', 'chemical', 'equilibrium', 'for', 'temperatures', 'to', '15,000', 'k', 'and', 'for', 'pressures', 'from', '10', 'to', '10', 'atmospheres', 'also', 'included', 'are', 'charts', 'showing', 'the', 'composition', 'of', 'air', 'the', 'isentropic', 'exponent', 'and', 'the', 'speed', 'of', 'sound', 'these', 'charts', 'are', 'based', 'on', 'thermodynamic', 'data', 'calculated', 'by', 'the', 'national', 'bureau', 'of', 'standards']\n",
      "\u001b[1mAfter blank space token removal : \u001b[0m ['free-flight', 'measurements', 'of', 'the', 'static', 'and', 'dynamic', 'charts', 'have', 'been', 'prepared', 'relating', 'the', 'thermodynamic', 'properties', 'of', 'air', 'in', 'chemical', 'equilibrium', 'for', 'temperatures', 'to', '15,000', 'k', 'and', 'for', 'pressures', 'from', '10', 'to', '10', 'atmospheres', 'also', 'included', 'are', 'charts', 'showing', 'the', 'composition', 'of', 'air', 'the', 'isentropic', 'exponent', 'and', 'the', 'speed', 'of', 'sound', 'these', 'charts', 'are', 'based', 'on', 'thermodynamic', 'data', 'calculated', 'by', 'the', 'national', 'bureau', 'of', 'standards']\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "Printing Statement : 3\n",
      "\u001b[1mBefore lower case text : \u001b[0m \n",
      "the operation of the npl 18in x 14in. wind tunnel in the transonic speed\n",
      " range .\n",
      " \n",
      "a brief description of the slotted liners used is given together with\n",
      "the power requirements and some flow surveys .\n",
      "some observations are made on wall interference on a half-model of a\n",
      "swept wing tested in the wind tunnel .\n",
      "\n",
      "\u001b[1mAfter lower case text and before tokenization : \u001b[0m \n",
      "the operation of the npl 18in x 14in. wind tunnel in the transonic speed\n",
      " range .\n",
      " \n",
      "a brief description of the slotted liners used is given together with\n",
      "the power requirements and some flow surveys .\n",
      "some observations are made on wall interference on a half-model of a\n",
      "swept wing tested in the wind tunnel .\n",
      "\n",
      "\u001b[1mAfter tokenization and before stopwords removal : \u001b[0m ['the', 'operation', 'of', 'the', 'npl', '18in', 'x', '14in', '.', 'wind', 'tunnel', 'in', 'the', 'transonic', 'speed', 'range', '.', 'a', 'brief', 'description', 'of', 'the', 'slotted', 'liners', 'used', 'is', 'given', 'together', 'with', 'the', 'power', 'requirements', 'and', 'some', 'flow', 'surveys', '.', 'some', 'observations', 'are', 'made', 'on', 'wall', 'interference', 'on', 'a', 'half-model', 'of', 'a', 'swept', 'wing', 'tested', 'in', 'the', 'wind', 'tunnel', '.']\n",
      "\u001b[1mAfter stopwords removal and before punctuations removal : \u001b[0m ['operation', 'npl', '18in', 'x', '14in', '.', 'wind', 'tunnel', 'transonic', 'speed', 'range', '.', 'brief', 'description', 'slotted', 'liners', 'used', 'given', 'together', 'power', 'requirements', 'flow', 'surveys', '.', 'observations', 'made', 'wall', 'interference', 'half-model', 'swept', 'wing', 'tested', 'wind', 'tunnel', '.']\n",
      "\u001b[1mAfter punctuations and before blank space token removal : \u001b[0m ['the', 'operation', 'of', 'the', 'npl', '18in', 'x', '14in', 'wind', 'tunnel', 'in', 'the', 'transonic', 'speed', 'range', 'a', 'brief', 'description', 'of', 'the', 'slotted', 'liners', 'used', 'is', 'given', 'together', 'with', 'the', 'power', 'requirements', 'and', 'some', 'flow', 'surveys', 'some', 'observations', 'are', 'made', 'on', 'wall', 'interference', 'on', 'a', 'half-model', 'of', 'a', 'swept', 'wing', 'tested', 'in', 'the', 'wind', 'tunnel']\n",
      "\u001b[1mAfter blank space token removal : \u001b[0m ['the', 'operation', 'of', 'the', 'npl', '18in', 'x', '14in', 'wind', 'tunnel', 'in', 'the', 'transonic', 'speed', 'range', 'a', 'brief', 'description', 'of', 'the', 'slotted', 'liners', 'used', 'is', 'given', 'together', 'with', 'the', 'power', 'requirements', 'and', 'some', 'flow', 'surveys', 'some', 'observations', 'are', 'made', 'on', 'wall', 'interference', 'on', 'a', 'half-model', 'of', 'a', 'swept', 'wing', 'tested', 'in', 'the', 'wind', 'tunnel']\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "Printing Statement : 4\n",
      "\u001b[1mBefore lower case text : \u001b[0m \n",
      "ablation of glassy materials around blunt bodies of\n",
      "revolution .\n",
      " \n",
      "  the steady-state equations of motion for a\n",
      "thin layer of an incompressible glassy material on the\n",
      "surface of an ablating and radiating blunt\n",
      "body are reduced to a first-order ordinary differential\n",
      "equation which is integrated numerically .\n",
      "this solution is coupled with the solution of the air\n",
      "boundary layer for both laminar and turbulent\n",
      "heat transfer with or without mass vaporization of\n",
      "the ablating material .  the distribution of the\n",
      "effective energy of ablation around the body is thus\n",
      "obtained for a cone cylinder with a hemispherical\n",
      "cap that re-enters the atmosphere at hypersonic\n",
      "flight speeds, and has quartz as the ablating\n",
      "material .  it is found that the ablation process from\n",
      "turbulent heating is more efficient than from\n",
      "the laminar case because of increased vaporization .\n",
      "this solution of the equations of motion at the\n",
      "stagnation point has been verified by are wind tunnel\n",
      "experiments .  the present state of development\n",
      "of the are wind tunnel does not permit its use for\n",
      "experimental investigations of ablation around\n",
      "blunt bodies under turbulent heating .\n",
      "\n",
      "\u001b[1mAfter lower case text and before tokenization : \u001b[0m \n",
      "ablation of glassy materials around blunt bodies of\n",
      "revolution .\n",
      " \n",
      "  the steady-state equations of motion for a\n",
      "thin layer of an incompressible glassy material on the\n",
      "surface of an ablating and radiating blunt\n",
      "body are reduced to a first-order ordinary differential\n",
      "equation which is integrated numerically .\n",
      "this solution is coupled with the solution of the air\n",
      "boundary layer for both laminar and turbulent\n",
      "heat transfer with or without mass vaporization of\n",
      "the ablating material .  the distribution of the\n",
      "effective energy of ablation around the body is thus\n",
      "obtained for a cone cylinder with a hemispherical\n",
      "cap that re-enters the atmosphere at hypersonic\n",
      "flight speeds, and has quartz as the ablating\n",
      "material .  it is found that the ablation process from\n",
      "turbulent heating is more efficient than from\n",
      "the laminar case because of increased vaporization .\n",
      "this solution of the equations of motion at the\n",
      "stagnation point has been verified by are wind tunnel\n",
      "experiments .  the present state of development\n",
      "of the are wind tunnel does not permit its use for\n",
      "experimental investigations of ablation around\n",
      "blunt bodies under turbulent heating .\n",
      "\n",
      "\u001b[1mAfter tokenization and before stopwords removal : \u001b[0m ['ablation', 'of', 'glassy', 'materials', 'around', 'blunt', 'bodies', 'of', 'revolution', '.', 'the', 'steady-state', 'equations', 'of', 'motion', 'for', 'a', 'thin', 'layer', 'of', 'an', 'incompressible', 'glassy', 'material', 'on', 'the', 'surface', 'of', 'an', 'ablating', 'and', 'radiating', 'blunt', 'body', 'are', 'reduced', 'to', 'a', 'first-order', 'ordinary', 'differential', 'equation', 'which', 'is', 'integrated', 'numerically', '.', 'this', 'solution', 'is', 'coupled', 'with', 'the', 'solution', 'of', 'the', 'air', 'boundary', 'layer', 'for', 'both', 'laminar', 'and', 'turbulent', 'heat', 'transfer', 'with', 'or', 'without', 'mass', 'vaporization', 'of', 'the', 'ablating', 'material', '.', 'the', 'distribution', 'of', 'the', 'effective', 'energy', 'of', 'ablation', 'around', 'the', 'body', 'is', 'thus', 'obtained', 'for', 'a', 'cone', 'cylinder', 'with', 'a', 'hemispherical', 'cap', 'that', 're-enters', 'the', 'atmosphere', 'at', 'hypersonic', 'flight', 'speeds', ',', 'and', 'has', 'quartz', 'as', 'the', 'ablating', 'material', '.', 'it', 'is', 'found', 'that', 'the', 'ablation', 'process', 'from', 'turbulent', 'heating', 'is', 'more', 'efficient', 'than', 'from', 'the', 'laminar', 'case', 'because', 'of', 'increased', 'vaporization', '.', 'this', 'solution', 'of', 'the', 'equations', 'of', 'motion', 'at', 'the', 'stagnation', 'point', 'has', 'been', 'verified', 'by', 'are', 'wind', 'tunnel', 'experiments', '.', 'the', 'present', 'state', 'of', 'development', 'of', 'the', 'are', 'wind', 'tunnel', 'does', 'not', 'permit', 'its', 'use', 'for', 'experimental', 'investigations', 'of', 'ablation', 'around', 'blunt', 'bodies', 'under', 'turbulent', 'heating', '.']\n",
      "\u001b[1mAfter stopwords removal and before punctuations removal : \u001b[0m ['ablation', 'glassy', 'materials', 'around', 'blunt', 'bodies', 'revolution', '.', 'steady-state', 'equations', 'motion', 'thin', 'layer', 'incompressible', 'glassy', 'material', 'surface', 'ablating', 'radiating', 'blunt', 'body', 'reduced', 'first-order', 'ordinary', 'differential', 'equation', 'integrated', 'numerically', '.', 'solution', 'coupled', 'solution', 'air', 'boundary', 'layer', 'laminar', 'turbulent', 'heat', 'transfer', 'without', 'mass', 'vaporization', 'ablating', 'material', '.', 'distribution', 'effective', 'energy', 'ablation', 'around', 'body', 'thus', 'obtained', 'cone', 'cylinder', 'hemispherical', 'cap', 're-enters', 'atmosphere', 'hypersonic', 'flight', 'speeds', ',', 'quartz', 'ablating', 'material', '.', 'found', 'ablation', 'process', 'turbulent', 'heating', 'efficient', 'laminar', 'case', 'increased', 'vaporization', '.', 'solution', 'equations', 'motion', 'stagnation', 'point', 'verified', 'wind', 'tunnel', 'experiments', '.', 'present', 'state', 'development', 'wind', 'tunnel', 'permit', 'use', 'experimental', 'investigations', 'ablation', 'around', 'blunt', 'bodies', 'turbulent', 'heating', '.']\n",
      "\u001b[1mAfter punctuations and before blank space token removal : \u001b[0m ['ablation', 'of', 'glassy', 'materials', 'around', 'blunt', 'bodies', 'of', 'revolution', 'the', 'steady-state', 'equations', 'of', 'motion', 'for', 'a', 'thin', 'layer', 'of', 'an', 'incompressible', 'glassy', 'material', 'on', 'the', 'surface', 'of', 'an', 'ablating', 'and', 'radiating', 'blunt', 'body', 'are', 'reduced', 'to', 'a', 'first-order', 'ordinary', 'differential', 'equation', 'which', 'is', 'integrated', 'numerically', 'this', 'solution', 'is', 'coupled', 'with', 'the', 'solution', 'of', 'the', 'air', 'boundary', 'layer', 'for', 'both', 'laminar', 'and', 'turbulent', 'heat', 'transfer', 'with', 'or', 'without', 'mass', 'vaporization', 'of', 'the', 'ablating', 'material', 'the', 'distribution', 'of', 'the', 'effective', 'energy', 'of', 'ablation', 'around', 'the', 'body', 'is', 'thus', 'obtained', 'for', 'a', 'cone', 'cylinder', 'with', 'a', 'hemispherical', 'cap', 'that', 're-enters', 'the', 'atmosphere', 'at', 'hypersonic', 'flight', 'speeds', 'and', 'has', 'quartz', 'as', 'the', 'ablating', 'material', 'it', 'is', 'found', 'that', 'the', 'ablation', 'process', 'from', 'turbulent', 'heating', 'is', 'more', 'efficient', 'than', 'from', 'the', 'laminar', 'case', 'because', 'of', 'increased', 'vaporization', 'this', 'solution', 'of', 'the', 'equations', 'of', 'motion', 'at', 'the', 'stagnation', 'point', 'has', 'been', 'verified', 'by', 'are', 'wind', 'tunnel', 'experiments', 'the', 'present', 'state', 'of', 'development', 'of', 'the', 'are', 'wind', 'tunnel', 'does', 'not', 'permit', 'its', 'use', 'for', 'experimental', 'investigations', 'of', 'ablation', 'around', 'blunt', 'bodies', 'under', 'turbulent', 'heating']\n",
      "\u001b[1mAfter blank space token removal : \u001b[0m ['ablation', 'of', 'glassy', 'materials', 'around', 'blunt', 'bodies', 'of', 'revolution', 'the', 'steady-state', 'equations', 'of', 'motion', 'for', 'a', 'thin', 'layer', 'of', 'an', 'incompressible', 'glassy', 'material', 'on', 'the', 'surface', 'of', 'an', 'ablating', 'and', 'radiating', 'blunt', 'body', 'are', 'reduced', 'to', 'a', 'first-order', 'ordinary', 'differential', 'equation', 'which', 'is', 'integrated', 'numerically', 'this', 'solution', 'is', 'coupled', 'with', 'the', 'solution', 'of', 'the', 'air', 'boundary', 'layer', 'for', 'both', 'laminar', 'and', 'turbulent', 'heat', 'transfer', 'with', 'or', 'without', 'mass', 'vaporization', 'of', 'the', 'ablating', 'material', 'the', 'distribution', 'of', 'the', 'effective', 'energy', 'of', 'ablation', 'around', 'the', 'body', 'is', 'thus', 'obtained', 'for', 'a', 'cone', 'cylinder', 'with', 'a', 'hemispherical', 'cap', 'that', 're-enters', 'the', 'atmosphere', 'at', 'hypersonic', 'flight', 'speeds', 'and', 'has', 'quartz', 'as', 'the', 'ablating', 'material', 'it', 'is', 'found', 'that', 'the', 'ablation', 'process', 'from', 'turbulent', 'heating', 'is', 'more', 'efficient', 'than', 'from', 'the', 'laminar', 'case', 'because', 'of', 'increased', 'vaporization', 'this', 'solution', 'of', 'the', 'equations', 'of', 'motion', 'at', 'the', 'stagnation', 'point', 'has', 'been', 'verified', 'by', 'are', 'wind', 'tunnel', 'experiments', 'the', 'present', 'state', 'of', 'development', 'of', 'the', 'are', 'wind', 'tunnel', 'does', 'not', 'permit', 'its', 'use', 'for', 'experimental', 'investigations', 'of', 'ablation', 'around', 'blunt', 'bodies', 'under', 'turbulent', 'heating']\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n",
      "Printing Statement : 5\n",
      "\u001b[1mBefore lower case text : \u001b[0m \n",
      "buckling of sandwich under normal pressure .\n",
      " \n",
      "  a theoretical study is made of the buckling of a sandwich sphere\n",
      "comprised of a core layer of low-modulus material and two thin\n",
      "facing layers of higher modulus material .  the solution for the\n",
      "buckling resistance of the sphere under normal external pressure\n",
      "is obtained by linearized theory, and is reducible to the classical\n",
      "solution for monocoque spherical shells .  critical buckling pressures\n",
      "are calculated for various radius-thickness ratios and sphere\n",
      "materials .\n",
      "\n",
      "\u001b[1mAfter lower case text and before tokenization : \u001b[0m \n",
      "buckling of sandwich under normal pressure .\n",
      " \n",
      "  a theoretical study is made of the buckling of a sandwich sphere\n",
      "comprised of a core layer of low-modulus material and two thin\n",
      "facing layers of higher modulus material .  the solution for the\n",
      "buckling resistance of the sphere under normal external pressure\n",
      "is obtained by linearized theory, and is reducible to the classical\n",
      "solution for monocoque spherical shells .  critical buckling pressures\n",
      "are calculated for various radius-thickness ratios and sphere\n",
      "materials .\n",
      "\n",
      "\u001b[1mAfter tokenization and before stopwords removal : \u001b[0m ['buckling', 'of', 'sandwich', 'under', 'normal', 'pressure', '.', 'a', 'theoretical', 'study', 'is', 'made', 'of', 'the', 'buckling', 'of', 'a', 'sandwich', 'sphere', 'comprised', 'of', 'a', 'core', 'layer', 'of', 'low-modulus', 'material', 'and', 'two', 'thin', 'facing', 'layers', 'of', 'higher', 'modulus', 'material', '.', 'the', 'solution', 'for', 'the', 'buckling', 'resistance', 'of', 'the', 'sphere', 'under', 'normal', 'external', 'pressure', 'is', 'obtained', 'by', 'linearized', 'theory', ',', 'and', 'is', 'reducible', 'to', 'the', 'classical', 'solution', 'for', 'monocoque', 'spherical', 'shells', '.', 'critical', 'buckling', 'pressures', 'are', 'calculated', 'for', 'various', 'radius-thickness', 'ratios', 'and', 'sphere', 'materials', '.']\n",
      "\u001b[1mAfter stopwords removal and before punctuations removal : \u001b[0m ['buckling', 'sandwich', 'normal', 'pressure', '.', 'theoretical', 'study', 'made', 'buckling', 'sandwich', 'sphere', 'comprised', 'core', 'layer', 'low-modulus', 'material', 'two', 'thin', 'facing', 'layers', 'higher', 'modulus', 'material', '.', 'solution', 'buckling', 'resistance', 'sphere', 'normal', 'external', 'pressure', 'obtained', 'linearized', 'theory', ',', 'reducible', 'classical', 'solution', 'monocoque', 'spherical', 'shells', '.', 'critical', 'buckling', 'pressures', 'calculated', 'various', 'radius-thickness', 'ratios', 'sphere', 'materials', '.']\n",
      "\u001b[1mAfter punctuations and before blank space token removal : \u001b[0m ['buckling', 'of', 'sandwich', 'under', 'normal', 'pressure', 'a', 'theoretical', 'study', 'is', 'made', 'of', 'the', 'buckling', 'of', 'a', 'sandwich', 'sphere', 'comprised', 'of', 'a', 'core', 'layer', 'of', 'low-modulus', 'material', 'and', 'two', 'thin', 'facing', 'layers', 'of', 'higher', 'modulus', 'material', 'the', 'solution', 'for', 'the', 'buckling', 'resistance', 'of', 'the', 'sphere', 'under', 'normal', 'external', 'pressure', 'is', 'obtained', 'by', 'linearized', 'theory', 'and', 'is', 'reducible', 'to', 'the', 'classical', 'solution', 'for', 'monocoque', 'spherical', 'shells', 'critical', 'buckling', 'pressures', 'are', 'calculated', 'for', 'various', 'radius-thickness', 'ratios', 'and', 'sphere', 'materials']\n",
      "\u001b[1mAfter blank space token removal : \u001b[0m ['buckling', 'of', 'sandwich', 'under', 'normal', 'pressure', 'a', 'theoretical', 'study', 'is', 'made', 'of', 'the', 'buckling', 'of', 'a', 'sandwich', 'sphere', 'comprised', 'of', 'a', 'core', 'layer', 'of', 'low-modulus', 'material', 'and', 'two', 'thin', 'facing', 'layers', 'of', 'higher', 'modulus', 'material', 'the', 'solution', 'for', 'the', 'buckling', 'resistance', 'of', 'the', 'sphere', 'under', 'normal', 'external', 'pressure', 'is', 'obtained', 'by', 'linearized', 'theory', 'and', 'is', 'reducible', 'to', 'the', 'classical', 'solution', 'for', 'monocoque', 'spherical', 'shells', 'critical', 'buckling', 'pressures', 'are', 'calculated', 'for', 'various', 'radius-thickness', 'ratios', 'and', 'sphere', 'materials']\n",
      "**********************************************************************************************\n",
      "**********************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "idtoName = {}\n",
    "files = os.listdir(\"Dataset\")\n",
    "count = 0\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    idtoName[i] = file\n",
    "    path = os.path.join(os.getcwd(), \"Dataset\", file)\n",
    "    \n",
    "    with open(path) as fp:\n",
    "        text = fp.read()\n",
    "        if count < 5:\n",
    "            print(\"**********************************************************************************************\")\n",
    "            print(\"**********************************************************************************************\")\n",
    "            print(f\"Printing Statement : {count+1}\")\n",
    "            L.append(preprocess(text, True))\n",
    "            print(\"**********************************************************************************************\")\n",
    "            print(\"**********************************************************************************************\")\n",
    "            count += 1\n",
    "        else:\n",
    "            L.append(preprocess(text, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIDf:\n",
    "    \n",
    "    def __init__(self, tokens, tf_weighting = \"log_norm\"):\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.idf = {}\n",
    "        self.tf_idf = None\n",
    "        self.v = None\n",
    "        self.n = len(tokens)\n",
    "        self.vocab = None\n",
    "        self.tf_weighting = tf_weighting\n",
    "        self.calc_tf_idf()\n",
    "        \n",
    "    def normalize_tf(self, tf, doc):\n",
    "        # tf = 0 will not occur. We are not computing for those terms.\n",
    "        if self.tf_weighting == 'binary':\n",
    "            return 1\n",
    "        \n",
    "        elif self.tf_weighting == 'raw_count':\n",
    "            return tf\n",
    "        \n",
    "        elif self.tf_weighting == 'term_frequency':\n",
    "            return tf / (len(doc))\n",
    "        \n",
    "        elif self.tf_weighting == 'log_norm':\n",
    "            return np.log ( 1 + tf)\n",
    "        \n",
    "        else:\n",
    "            return 0.5 + 0.5 * (tf / max(list(Counter(doc).values())))\n",
    "            \n",
    "        \n",
    "    def calc_tf_idf(self):\n",
    "        \n",
    "        term_freq = {}\n",
    "        # Go through each document. Create frequency dictionary for each doc\n",
    "        # Update term frequency dictionary with raw count.\n",
    "        for i in range(self.n):\n",
    "            freq = Counter(self.tokens[i])\n",
    "            for term in freq.elements():\n",
    "                if term not in term_freq:\n",
    "                    term_freq[term] = {i:freq[term]}\n",
    "                else:\n",
    "                    term_freq[term][i] = freq[term]\n",
    "        \n",
    "        self.v = len(term_freq)\n",
    "        self.vocab = list(term_freq.keys())\n",
    "        \n",
    "        # Create idf\n",
    "        for term in term_freq:\n",
    "            self.idf[term] = np.log(self.n/len(term_freq[term]) + 1)\n",
    "        \n",
    "        # Initialize TF-IDF\n",
    "        self.tf_idf = np.zeros((self.n, self.v))\n",
    "        \n",
    "        # Calculate TF-IDF for terms which are present in doc else tf-idf is always 0.\n",
    "        for term in term_freq:\n",
    "            for doc in term_freq[term]:\n",
    "                tf =  self.normalize_tf(term_freq[term][doc], self.tokens[doc])\n",
    "                self.tf_idf[doc][self.vocab.index(term)] = tf * self.idf[term]\n",
    "    \n",
    "    def query(self, query_doc, num_doc = 5):\n",
    "        \n",
    "        q_freq = Counter(query_doc)\n",
    "        q_vec = np.zeros(self.v).reshape(-1,1)\n",
    "\n",
    "        for term in q_freq:\n",
    "            if term in self.vocab:\n",
    "                tf = self.normalize_tf(q_freq[term], query_doc)\n",
    "                q_vec[self.vocab.index(term)] = tf * self.idf[term]\n",
    "                \n",
    "        # Dot product of tf-idf and query vector\n",
    "        # Sort it in descending order along with its document id\n",
    "        # Result is (score, doc-id)\n",
    "        if norm(q_vec) == 0 or norm(self.tf_idf) == 0:\n",
    "            result = sorted(zip((self.tf_idf@q_vec).flatten(), np.arange(self.n)), reverse = True, key = lambda x: x[0])\n",
    "        else:\n",
    "            result = sorted(zip(((self.tf_idf@q_vec)/(norm(self.tf_idf)*norm(q_vec))).flatten(), np.arange(self.n)), reverse = True, key = lambda x: x[0])\n",
    "\n",
    "\n",
    "        return result[0:num_doc]\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_tf_idf_score(queries):\n",
    "    binary_obj = TfIDf(L,\"binary\")\n",
    "    raw_count_obj = TfIDf(L,\"raw_count\")\n",
    "    term_frequency_obj = TfIDf(L,\"term_frequency\")\n",
    "    log_norm_obj = TfIDf(L,\"log_norm\")\n",
    "    double_norm_obj = TfIDf(L,\"double_norm\")\n",
    "    df_list = []\n",
    "    for query in queries:\n",
    "        query_token = preprocess(query, False)\n",
    "        ll = []\n",
    "        for obj in [binary_obj, raw_count_obj, term_frequency_obj, log_norm_obj, double_norm_obj]:\n",
    "            result = obj.query(query_token, 5)\n",
    "            df = pd.DataFrame(result, columns=['Score', 'Document'])\n",
    "            df['Document'] = df['Document'].apply(lambda x: idtoName[x])\n",
    "            ll.append(df)\n",
    "        df_list.append(ll)\n",
    "    return df_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries. 1\n",
      "Query : Which document should this query belong to? Maybe our algorithm can help figure it out.\n",
      "Query 1:  Which document should this query belong to? Maybe our algorithm can help figure it out.\n",
      "Documents retrieved for query 1 for binary: \n",
      " +----+------------+---------------+\n",
      "|    |      Score | Document      |\n",
      "|----+------------+---------------|\n",
      "|  0 | 0.00353159 | cranfield0153 |\n",
      "|  1 | 0.00319108 | cranfield0928 |\n",
      "|  2 | 0.00277138 | cranfield0962 |\n",
      "|  3 | 0.00250726 | cranfield0798 |\n",
      "|  4 | 0.00250726 | cranfield0499 |\n",
      "+----+------------+---------------+\n",
      "Documents retrieved for query 1 for raw count: \n",
      " +----+------------+---------------+\n",
      "|    |      Score | Document      |\n",
      "|----+------------+---------------|\n",
      "|  0 | 0.00359466 | cranfield0962 |\n",
      "|  1 | 0.00307852 | cranfield0928 |\n",
      "|  2 | 0.00295812 | cranfield0914 |\n",
      "|  3 | 0.00253147 | cranfield0798 |\n",
      "|  4 | 0.00250967 | cranfield1147 |\n",
      "+----+------------+---------------+\n",
      "Documents retrieved for query 1 for term frequency: \n",
      " +----+------------+---------------+\n",
      "|    |      Score | Document      |\n",
      "|----+------------+---------------|\n",
      "|  0 | 0.00454263 | cranfield0153 |\n",
      "|  1 | 0.00194453 | cranfield0914 |\n",
      "|  2 | 0.00184536 | cranfield0130 |\n",
      "|  3 | 0.00162415 | cranfield0463 |\n",
      "|  4 | 0.00155529 | cranfield0457 |\n",
      "+----+------------+---------------+\n",
      "Documents retrieved for query 1 for log normalization: \n",
      " +----+------------+---------------+\n",
      "|    |      Score | Document      |\n",
      "|----+------------+---------------|\n",
      "|  0 | 0.00334915 | cranfield0962 |\n",
      "|  1 | 0.00325503 | cranfield0928 |\n",
      "|  2 | 0.00306321 | cranfield0914 |\n",
      "|  3 | 0.00298822 | cranfield0153 |\n",
      "|  4 | 0.00259431 | cranfield0798 |\n",
      "+----+------------+---------------+\n",
      "Documents retrieved for query 1 for double normalization: \n",
      " +----+------------+---------------+\n",
      "|    |      Score | Document      |\n",
      "|----+------------+---------------|\n",
      "|  0 | 0.00385409 | cranfield0153 |\n",
      "|  1 | 0.00302948 | cranfield0928 |\n",
      "|  2 | 0.00267723 | cranfield0962 |\n",
      "|  3 | 0.00245303 | cranfield0914 |\n",
      "|  4 | 0.0023455  | cranfield0499 |\n",
      "+----+------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries. \"))\n",
    "count = 1\n",
    "queries = []\n",
    "operand = []\n",
    "while count <= N:\n",
    "    query = input(\"Query : \")\n",
    "    queries.append(query)\n",
    "    count +=1\n",
    "\n",
    "df_list = queries_tf_idf_score(queries)\n",
    "for idx in range(len(queries)):\n",
    "    print(f\"Query {idx + 1}: \", queries[idx])\n",
    "    print(f\"Documents retrieved for query {idx + 1} for binary: \\n\", \n",
    "          tabulate(df_list[idx][0], headers='keys', tablefmt='psql'))\n",
    "    print(f\"Documents retrieved for query {idx + 1} for raw count: \\n\", \n",
    "          tabulate(df_list[idx][1], headers='keys', tablefmt='psql'))\n",
    "    print(f\"Documents retrieved for query {idx + 1} for term frequency: \\n\", \n",
    "          tabulate(df_list[idx][2], headers='keys', tablefmt='psql'))\n",
    "    print(f\"Documents retrieved for query {idx + 1} for log normalization: \\n\", \n",
    "          tabulate(df_list[idx][3], headers='keys', tablefmt='psql'))\n",
    "    print(f\"Documents retrieved for query {idx + 1} for double normalization: \\n\", \n",
    "          tabulate(df_list[idx][4], headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(queries):\n",
    "    sets_doc ={}\n",
    "    for i in range(len(L)):\n",
    "        sets_doc[i] = set(L[i])\n",
    "    df_list = []\n",
    "    for query in queries:\n",
    "        query_token = preprocess(query, False)\n",
    "        query_token = set(query_token)\n",
    "        jc = np.zeros(len(L))\n",
    "        for doc in sets_doc:\n",
    "            jc[doc] = len(sets_doc[doc] & query_token) / len(sets_doc[doc] | query_token)\n",
    "        result = sorted(zip(jc, np.arange(len(L))), reverse = True, key = lambda x: x[0])\n",
    "        df = pd.DataFrame(result[0:10], columns=['Score', 'Document'])\n",
    "        df['Document'] = df['Document'].apply(lambda x: idtoName[x])\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries.1\n",
      "Query : Which document should this query belong to? Maybe our algorithm can help figure it out.\n",
      "Query 1:  Which document should this query belong to? Maybe our algorithm can help figure it out.\n",
      "Documents retrieved for query 1 using Jaccard Coefficient: \n",
      " +----+-----------+---------------+\n",
      "|    |     Score | Document      |\n",
      "|----+-----------+---------------|\n",
      "|  0 | 0.1       | cranfield0281 |\n",
      "|  1 | 0.0754717 | cranfield0835 |\n",
      "|  2 | 0.0740741 | cranfield0153 |\n",
      "|  3 | 0.0704225 | cranfield1275 |\n",
      "|  4 | 0.0694444 | cranfield0637 |\n",
      "|  5 | 0.0677966 | cranfield0855 |\n",
      "|  6 | 0.0666667 | cranfield0909 |\n",
      "|  7 | 0.0657895 | cranfield1388 |\n",
      "|  8 | 0.0638298 | cranfield0506 |\n",
      "|  9 | 0.0632911 | cranfield0469 |\n",
      "+----+-----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries.\"))\n",
    "count = 1\n",
    "queries = []\n",
    "operand = []\n",
    "while count <= N:\n",
    "    query = input(\"Query : \")\n",
    "    queries.append(query)\n",
    "    count +=1\n",
    "\n",
    "df_list = jaccard_coefficient(queries)\n",
    "for idx in range(len(queries)):\n",
    "    print(f\"Query {idx + 1}: \", queries[idx])\n",
    "    print(f\"Documents retrieved for query {idx + 1} using Jaccard Coefficient: \\n\", \n",
    "          tabulate(df_list[idx], headers='keys', tablefmt='psql'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3ec7de9268a0b79728a60b875e56ac0966e701affb38f43c868f58afb5023e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
