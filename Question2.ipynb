{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy.linalg import norm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import string\n",
    "import math\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BBC News Train.csv\", sep = \",\")\n",
    "df.drop(['ArticleId'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Category\n",
       "0  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1  german business confidence slides german busin...  business\n",
       "2  bbc poll indicates economic gloom citizens in ...  business\n",
       "3  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [preprocess(text) for text in df[\"Text\"]]\n",
    "y = df['Category'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category = ['business',\n",
    "            'tech',\n",
    "            'politics',\n",
    "            'sport',\n",
    "            'entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfICf:\n",
    "    def __init__(self):             \n",
    "        self.index_dict = {}\n",
    "        self.icf = {}\n",
    "        self.tf = {}\n",
    "        self.class_freq = {}\n",
    "        self.tficf_score = {}\n",
    "\n",
    "    def TF(self, corpus, category):\n",
    "        X = np.array(corpus, dtype = object)\n",
    "        for label in self.classes:\n",
    "            idx = (category == label)\n",
    "            C = sum(X[idx], [])\n",
    "            count = Counter(C)\n",
    "            self.tf[label] = count\n",
    "\n",
    "    def CF(self, corpus, category):\n",
    "        X = np.array(corpus, dtype = object)\n",
    "        for label in self.classes:\n",
    "            idx = (category == label)\n",
    "            C = sum(X[idx], [])\n",
    "            for term in self.vocab:\n",
    "                if term in C:\n",
    "                    try:\n",
    "                        self.class_freq[term] += 1\n",
    "                    except:\n",
    "                        self.class_freq[term] = 1\n",
    "\n",
    "    def tficfScore(self):\n",
    "        for label in self.classes:\n",
    "            tficf_class_score = {}\n",
    "            for term in self.vocab:\n",
    "                try:\n",
    "                    tficf_class_score[term] = self.icf[term] * self.tf[label][term]\n",
    "                except:\n",
    "                    tficf_class_score[term] = 0\n",
    "            \n",
    "            self.tficf_score[label] = tficf_class_score           \n",
    "\n",
    "\n",
    "    def fit(self, corpus, category):\n",
    "        assert type(category).__module__ == np.__name__\n",
    "        assert isinstance(corpus, list)\n",
    "        \n",
    "        self.vocab = set.union(*map(set, corpus))\n",
    "        self.classes = np.unique(category)\n",
    "        self.N_docs = len(corpus)\n",
    "        num_classes = len(self.classes)\n",
    "\n",
    "        self.TF(corpus, category)\n",
    "        self.CF(corpus, category)\n",
    "\n",
    "        self.icf = {term : math.log(num_classes / self.class_freq[term]) for term in self.vocab}\n",
    "        self.index_dict = {term : idx for idx, term in enumerate(self.vocab)} \n",
    "        self.tficfScore()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:     \n",
    "    def __init__(self, vocab, tf_score, tf):\n",
    "        self.vocab = vocab\n",
    "        self.V = len(vocab)\n",
    "        self.tf_score = tf_score\n",
    "        self.tf = tf\n",
    "        self.Ny = {}\n",
    "\n",
    "    def fit(self, corpus, y):\n",
    "        assert type(y).__module__ == np.__name__\n",
    "        assert isinstance(corpus, list)\n",
    "        assert isinstance(corpus[0], list)\n",
    "        \n",
    "        X = np.array(corpus, dtype=object)\n",
    "\n",
    "        self.n_sample = y.shape[0]\n",
    "        self.classes = np.unique(y) \n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "        instances_per_class = Counter(y)\n",
    "        self.prior = {label : instances_per_class[label] / self.n_sample for label in self.classes}\n",
    "\n",
    "        for label in self.classes:\n",
    "            idx = (y == label)\n",
    "            C = set(sum(X[idx], []))\n",
    "            prob = 0\n",
    "            for term in C:\n",
    "                try:\n",
    "                    prob += self.tf_score[label][term] * self.tf[label][term] \n",
    "                except:\n",
    "                    pass\n",
    "            self.Ny[label] = prob\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for X_i in X:\n",
    "            pred = []\n",
    "            for label in self.classes:\n",
    "                Nyi = 0\n",
    "                for term in X_i:    \n",
    "                    try:\n",
    "                        Nyi += self.tf_score[label][term] * self.tf[label][term]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                probability = np.log(self.prior[label]) + np.log((Nyi + 1) / (self.Ny[label] + self.V))\n",
    "                pred.append(probability)\n",
    "\n",
    "            pred = np.array(pred)\n",
    "            y_pred.append(self.classes[np.argmax(pred)])\n",
    "                \n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naive Baye's with TF-ICF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>80 : 20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.94      0.94       269\n",
      "entertainment       0.91      0.94      0.93       218\n",
      "     politics       0.95      0.93      0.94       219\n",
      "        sport       0.98      1.00      0.99       277\n",
      "         tech       0.90      0.89      0.90       209\n",
      "\n",
      "     accuracy                           0.94      1192\n",
      "    macro avg       0.94      0.94      0.94      1192\n",
      " weighted avg       0.94      0.94      0.94      1192\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.91      0.93      0.92        67\n",
      "entertainment       0.90      0.96      0.93        55\n",
      "     politics       0.96      0.87      0.91        55\n",
      "        sport       0.94      0.99      0.96        69\n",
      "         tech       0.92      0.87      0.89        52\n",
      "\n",
      "     accuracy                           0.93       298\n",
      "    macro avg       0.93      0.92      0.92       298\n",
      " weighted avg       0.93      0.93      0.93       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "tficf = TfICf().fit(X_train, y_train)\n",
    "clf = NaiveBayes(tficf.vocab, tficf.tficf_score, tficf.tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>70 : 30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.93      0.93       235\n",
      "entertainment       0.91      0.94      0.92       191\n",
      "     politics       0.95      0.91      0.93       192\n",
      "        sport       0.96      1.00      0.98       242\n",
      "         tech       0.92      0.89      0.90       183\n",
      "\n",
      "     accuracy                           0.94      1043\n",
      "    macro avg       0.94      0.93      0.93      1043\n",
      " weighted avg       0.94      0.94      0.94      1043\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.93      0.93      0.93       101\n",
      "entertainment       0.91      0.94      0.92        82\n",
      "     politics       0.94      0.89      0.91        82\n",
      "        sport       0.93      0.98      0.95       104\n",
      "         tech       0.93      0.87      0.90        78\n",
      "\n",
      "     accuracy                           0.93       447\n",
      "    macro avg       0.93      0.92      0.92       447\n",
      " weighted avg       0.93      0.93      0.93       447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "tficf = TfICf().fit(X_train, y_train)\n",
    "clf = NaiveBayes(tficf.vocab, tficf.tficf_score, tficf.tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>60 : 40 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.92      0.93       201\n",
      "entertainment       0.91      0.93      0.92       164\n",
      "     politics       0.95      0.95      0.95       164\n",
      "        sport       0.97      0.98      0.97       208\n",
      "         tech       0.92      0.91      0.92       157\n",
      "\n",
      "     accuracy                           0.94       894\n",
      "    macro avg       0.94      0.94      0.94       894\n",
      " weighted avg       0.94      0.94      0.94       894\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.90      0.92       135\n",
      "entertainment       0.91      0.96      0.93       109\n",
      "     politics       0.94      0.92      0.93       110\n",
      "        sport       0.96      0.97      0.96       138\n",
      "         tech       0.91      0.91      0.91       104\n",
      "\n",
      "     accuracy                           0.93       596\n",
      "    macro avg       0.93      0.93      0.93       596\n",
      " weighted avg       0.93      0.93      0.93       596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "tficf = TfICf().fit(X_train, y_train)\n",
    "clf = NaiveBayes(tficf.vocab, tficf.tficf_score, tficf.tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>50 : 50 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.92      0.94       168\n",
      "entertainment       0.92      0.93      0.93       136\n",
      "     politics       0.96      0.96      0.96       137\n",
      "        sport       0.96      0.99      0.98       173\n",
      "         tech       0.93      0.92      0.93       131\n",
      "\n",
      "     accuracy                           0.95       745\n",
      "    macro avg       0.95      0.95      0.95       745\n",
      " weighted avg       0.95      0.95      0.95       745\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.89      0.91       168\n",
      "entertainment       0.91      0.99      0.95       137\n",
      "     politics       0.93      0.92      0.93       137\n",
      "        sport       0.96      0.99      0.98       173\n",
      "         tech       0.93      0.88      0.91       130\n",
      "\n",
      "     accuracy                           0.94       745\n",
      "    macro avg       0.93      0.93      0.93       745\n",
      " weighted avg       0.94      0.94      0.94       745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "tficf = TfICf().fit(X_train, y_train)\n",
    "clf = NaiveBayes(tficf.vocab, tficf.tficf_score, tficf.tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Baye's with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>80 : 20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.87      0.44      0.58       269\n",
      "entertainment       0.97      0.52      0.67       218\n",
      "     politics       0.34      0.99      0.50       219\n",
      "        sport       0.95      0.64      0.77       277\n",
      "         tech       0.94      0.49      0.65       209\n",
      "\n",
      "     accuracy                           0.61      1192\n",
      "    macro avg       0.81      0.62      0.63      1192\n",
      " weighted avg       0.82      0.61      0.64      1192\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.82      0.48      0.60        67\n",
      "entertainment       1.00      0.51      0.67        55\n",
      "     politics       0.33      1.00      0.50        55\n",
      "        sport       0.94      0.65      0.77        69\n",
      "         tech       0.89      0.31      0.46        52\n",
      "\n",
      "     accuracy                           0.59       298\n",
      "    macro avg       0.80      0.59      0.60       298\n",
      " weighted avg       0.80      0.59      0.61       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "corpus = [' '.join(X_i) for X_i in X_train]\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "score = tfidf.fit_transform(corpus).toarray()\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "tf_score = {}\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "tf_score = {}\n",
    "for label in classes:\n",
    "    C = score[label == y_train]\n",
    "    score_dict = {term : scr for term, scr in zip(vocab, np.mean(C, axis = 0))}\n",
    "    tf_score[label] = score_dict\n",
    "\n",
    "X_t = np.array(X_train, dtype = object)\n",
    "tf = {}\n",
    "for label in classes:\n",
    "    idx = (y_train == label)\n",
    "    C = sum(X_t[idx], [])\n",
    "    count = Counter(C)\n",
    "    tf[label] = count\n",
    "\n",
    "clf = NaiveBayes(vocab, tf_score, tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>70 : 30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.89      0.41      0.56       235\n",
      "entertainment       0.96      0.51      0.66       191\n",
      "     politics       0.33      0.99      0.49       192\n",
      "        sport       0.93      0.64      0.76       242\n",
      "         tech       0.93      0.45      0.61       183\n",
      "\n",
      "     accuracy                           0.60      1043\n",
      "    macro avg       0.81      0.60      0.62      1043\n",
      " weighted avg       0.82      0.60      0.62      1043\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.87      0.45      0.59       101\n",
      "entertainment       1.00      0.52      0.69        82\n",
      "     politics       0.33      1.00      0.49        82\n",
      "        sport       0.96      0.67      0.79       104\n",
      "         tech       0.93      0.33      0.49        78\n",
      "\n",
      "     accuracy                           0.60       447\n",
      "    macro avg       0.82      0.60      0.61       447\n",
      " weighted avg       0.82      0.60      0.62       447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "corpus = [' '.join(X_i) for X_i in X_train]\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "score = tfidf.fit_transform(corpus).toarray()\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "tf_score = {}\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "tf_score = {}\n",
    "for label in classes:\n",
    "    C = score[label == y_train]\n",
    "    score_dict = {term : scr for term, scr in zip(vocab, np.mean(C, axis = 0))}\n",
    "    tf_score[label] = score_dict\n",
    "\n",
    "X_t = np.array(X_train, dtype = object)\n",
    "tf = {}\n",
    "for label in classes:\n",
    "    idx = (y_train == label)\n",
    "    C = sum(X_t[idx], [])\n",
    "    count = Counter(C)\n",
    "    tf[label] = count\n",
    "\n",
    "clf = NaiveBayes(vocab, tf_score, tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>60 : 40 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.88      0.45      0.60       201\n",
      "entertainment       0.96      0.54      0.69       164\n",
      "     politics       0.34      0.99      0.50       164\n",
      "        sport       0.94      0.64      0.76       208\n",
      "         tech       0.96      0.46      0.63       157\n",
      "\n",
      "     accuracy                           0.61       894\n",
      "    macro avg       0.81      0.62      0.64       894\n",
      " weighted avg       0.82      0.61      0.64       894\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.86      0.44      0.58       135\n",
      "entertainment       1.00      0.52      0.69       109\n",
      "     politics       0.33      1.00      0.49       110\n",
      "        sport       0.95      0.66      0.78       138\n",
      "         tech       0.87      0.33      0.48       104\n",
      "\n",
      "     accuracy                           0.59       596\n",
      "    macro avg       0.80      0.59      0.60       596\n",
      " weighted avg       0.81      0.59      0.61       596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "corpus = [' '.join(X_i) for X_i in X_train]\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "score = tfidf.fit_transform(corpus).toarray()\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "tf_score = {}\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "tf_score = {}\n",
    "for label in classes:\n",
    "    C = score[label == y_train]\n",
    "    score_dict = {term : scr for term, scr in zip(vocab, np.mean(C, axis = 0))}\n",
    "    tf_score[label] = score_dict\n",
    "\n",
    "X_t = np.array(X_train, dtype = object)\n",
    "tf = {}\n",
    "for label in classes:\n",
    "    idx = (y_train == label)\n",
    "    C = sum(X_t[idx], [])\n",
    "    count = Counter(C)\n",
    "    tf[label] = count\n",
    "\n",
    "clf = NaiveBayes(vocab, tf_score, tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 50 : 50 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TRAINING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.88      0.42      0.56       168\n",
      "entertainment       0.96      0.51      0.67       136\n",
      "     politics       0.33      1.00      0.50       137\n",
      "        sport       0.96      0.62      0.76       173\n",
      "         tech       0.97      0.50      0.66       131\n",
      "\n",
      "     accuracy                           0.60       745\n",
      "    macro avg       0.82      0.61      0.63       745\n",
      " weighted avg       0.83      0.60      0.63       745\n",
      "\n",
      "****************************************************************************************************\n",
      "TESTING REPORT\n",
      "****************************************************************************************************\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.83      0.39      0.53       168\n",
      "entertainment       0.98      0.47      0.64       137\n",
      "     politics       0.31      0.99      0.47       137\n",
      "        sport       0.92      0.64      0.75       173\n",
      "         tech       0.89      0.32      0.47       130\n",
      "\n",
      "     accuracy                           0.56       745\n",
      "    macro avg       0.79      0.56      0.57       745\n",
      " weighted avg       0.80      0.56      0.58       745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50 , stratify=y, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "corpus = [' '.join(X_i) for X_i in X_train]\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "score = tfidf.fit_transform(corpus).toarray()\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "tf_score = {}\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "tf_score = {}\n",
    "for label in classes:\n",
    "    C = score[label == y_train]\n",
    "    score_dict = {term : scr for term, scr in zip(vocab, np.mean(C, axis = 0))}\n",
    "    tf_score[label] = score_dict\n",
    "\n",
    "X_t = np.array(X_train, dtype = object)\n",
    "tf = {}\n",
    "for label in classes:\n",
    "    idx = (y_train == label)\n",
    "    C = sum(X_t[idx], [])\n",
    "    count = Counter(C)\n",
    "    tf[label] = count\n",
    "\n",
    "clf = NaiveBayes(vocab, tf_score, tf).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"TRAINING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_train, y_train_pred, zero_division = 1))\n",
    "print('*' * 100)\n",
    "print(\"TESTING REPORT\")\n",
    "print('*' * 100)\n",
    "print(classification_report(y_test, y_test_pred, zero_division = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
